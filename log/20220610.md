### 文件系统

>  open、read、write、close

>  在 ch6 分支加入了对 k210 板的依赖，每次下载依赖都失败，因此直接在目录中添加本地依赖
>
>  easy-fs：从内核中独立出来的文件系统
>
>  easy-fs-fuse：将当前 OS 上的应用的可执行文件按照 easy-fs 的格式进行打包
>
>  loader：被移除，应用程序不需要事先链接到 kernel 的数据段中

### easyfs

> 1. 磁盘块设备接口层：读写磁盘块设备的trait接口，read_block 和 write_block 将数据以块为单位读取到内存缓冲区
> 2. 块缓存层：位于内存的磁盘块数据缓存，保证对同一个块的读写都是在缓冲区进行，不会重新从块设备读写，保证数据的一致性，实现了类似 FIFO 的缓存替换算法，块被换出时才会写到块设备中
> 3. 磁盘数据结构层：表示磁盘文件系统的数据结构，SuperBlock、inode 节点位图、数据块位图、inode 节点区、数据块区
> 4. 磁盘块管理器层：实现对磁盘文件系统的管理，create、open、alloc_inode、alloc_data、dealloc_data、dealloc_inode（还没有实现）
> 5. 索引节点层：实现文件创建/文件打开/文件读写等操作，主要是通过对文件的 inode 进行操作，inode.new、inode.find、inode.create、inode.read_at、inode.write_at

> 需要把文件纳入到进程的管理之中，进程会增加一个文件描述符表 fd_table 向量，之后对文件的操作将会通过对文件描述符 fd 来进行

### 对文件系统的简化

> 1. 只存在根目录 / ，其余的文件都放在根目录中，索引文件时直接使用文件名，不使用含有 / 的绝对路径
> 2. 只有单用户，也不限制访问权限
> 3. 不记录文件的时间戳
> 4. 不支持软硬链接
> 5. 只实现了部分系统调用

### fn sys_open(path: &str, flags: u32) -> isize

> 在进程的 fd_table 中找到一个空位，记录文件对应的 inode 节点编号，返回的是 fd_table 向量中的下标，不是 inode 节点编号，根据 flags 来设置进程对文件的访问权限，文件系统不设置权限，因此只需要在进程这一端设置权限

### 文件顺序读写

> 进程内部记录了文件偏移，通过这个 offset 来实现顺序读写和随机读写

### easy-fs 具体代码

###### block_dev.rs

> 只实现了 read_block 和 write_block 特性，具体的类型在 easy-fs-fuse 的 main.rs 中，直接将应用程序写到块设备中

###### block_cache.rs

```
pub struct BlockCache {
    cache: [u8; BLOCK_SZ],	// 内存中的缓存，仍然是看作一个 array
    block_id: usize,		// 缓存对应的块在块设备中的编号
    block_device: Arc<dyn BlockDevice>,  // 所属的块设备
    modified: bool,			// 文件是否被修改
}
```

> new(block_id, block_device) 新建一个 block_cache
>
> addr_of_offset(offset) 返回 cache 中对应 offset 位置的指针
>
> get_ref<T>(offset) 返回 cache 中 offset 位置的 T 类型的不可变引用
>
> get_mut<T>(offset) 同上，返回的是 T 类型的可变引用，同时这个块会被标记为 dirty
>
> read<T, V>(offset, f) 获取到  cache 中 offset 位置的不可变引用，然后执行传递进来闭包 f 中的函数
>
> modify<T, V>(offset, f) 同上，只不过是获取的可变引用
>
> sync() 只有在 drop 时才会调用，将缓冲区的内容写回到块设备中

```
pub struct BlockCacheManager {	// 全局的块缓冲区，队列实现 FIFO，保证了互斥
    queue: VecDeque<(usize, Arc<Mutex<BlockCache>>)>,
}		// 缓冲队列大小为 16
```

> get_block_cache(block, device) -> block_cache 如果在 BlockCacheManager 的队列中找到对应的 block_cache，则直接返回 block_cache 的 Arc 指针，如果没有找到，则判断缓冲队列是否已满，如果未满，直接插进队尾；如果满了，则找到一个引用计数为 1 的 block_cache 进行替换

> 暴露出来的接口 get_block_cache、block_cache_sync_all（只是将所有的块缓存同步到块设备，但是并不会丢掉这些缓存）

###### layout.rs 文件系统布局

> 先规定好根目录下的目录的最大数目，一级目录的数目，二级目录的数目，各级目录的 inode 节点编号的上界

```
#[repr(C)]		// SuperBlock 结构
pub struct SuperBlock {
    magic: u32,
    pub total_blocks: u32, 			// 一个块设备总共的块数目
    pub inode_bitmap_blocks: u32,	// inode 位图所占的块数目
    pub inode_area_blocks: u32,		// inode 区域占的块数目
    pub data_bitmap_blocks: u32,	// 数据区位图所占的块数目
    pub data_area_blocks: u32,		// 数据区的所占的块数目
}
```

```
#[repr(C)]				// 块设备中的 inode 数据结构，128 字节，每个块正好4个
pub struct DiskInode {
    pub size: u32,			// 文件或目录的大小
    pub direct: [u32; INODE_DIRECT_COUNT],	// 直接块的 block_id 向量
    pub indirect1: u32,		// 一级间接块所在的 block_id 编号
    pub indirect2: u32,		// 二级间接块所在的 block_id 编号
    type_: DiskInodeType,	// inode 类型
}
```

> blocks_num_needed(new_size) 根据 new_size 来判断还需要申请多少个 block
>
> get_block_id(inner_id, block_device) 获取 inode 中的 inner_id 对应的块在磁盘上的 id
>
> increase_size(new_size, new_blocks, block_device) 扩容，新创建了 inode 之后，需要通过 increase_size 来扩容
>
> clear_size(block_device) 清空文件的内容并回收所有数据和索引块
>
> read_at(offset, buf, block_devic) 将文件内容从 offset 字节开始的部分读到内存中的缓冲区 buf 中，并返回实际读到的字节数，若超出文件范围，则会直接返回 0
>
> write_at(offset, buf, block_devic) 将缓冲区中的数据写入到文件 offset 位置处，如果文件剩余空间不足，则会调用 increase_size 来扩容

```
#[repr(C)]		// 目录项实体，32 字节，一个 block 可以放 16 个目录项
pub struct DirEntry {
    name: [u8; NAME_LENGTH_LIMIT + 1],		// 文件名或目录名
    inode_number: u32,		// 目录文件所在的块编号
}
```

###### bitmap.rs 位图

> alloc(block_device) 从在位图中找到一个空闲位置，然后返回 bit 下标
>
> dealloc(block_device, bit) 释放掉 bit 表示的块，然后将对应的 bit 置 0  

###### efs.rs 磁盘块管理器

```
pub struct EasyFileSystem {		// easy-fs 整体布局
    pub block_device: Arc<dyn BlockDevice>,
    pub inode_bitmap: Bitmap,
    pub data_bitmap: Bitmap,
    inode_area_start_block: u32,
    data_area_start_block: u32,
}
```

> fn create(block_device, total_blocks, inode_bitmap_blocks) -> Arc<Mutex<Self>>
>
> 根据传递进来的参数，计算各个区域的大小，初始化超级块，创建根目录 “/”
>
> fn open(block_device) -> Arc<Mutex<Self>> 打开块设备，将块设备中的 easy-fs 挂载
>
> fn root_inode 返回根目录的 inode
>
> fn get_disk_inode_pos(inode_id) 获取 inode_id 对应的实际的位置
>
> fn get_data_block_id(block_id) 获取数据块实际的位置
>
> alloc_inode、alloc_data、dealloc_data 完成 inode 和数据块的分配和回收

###### vfs.rs 虚拟文件系统

```
pub struct Inode {			// 在内存中的 inode
    block_id: usize,
    block_offset: usize,
    fs: Arc<Mutex<EasyFileSystem>>,
    block_device: Arc<dyn BlockDevice>,
}
```

> read_disk_inode
>
> modify_disk_inode 两者都是调用一个函数来对块设备中的某个区域进行操作，并返回相应的数据结构
>
> find_inode_id(name, disk_inode) 在 disk_inode 上找到对应的 name 的 inode 编号
>
> find 在根目录中找 name 对应的 inode 编号
>
> increase_size 扩充当前的 inode
>
> create 在当前的 inode 下创建 inode
>
> ls 列举出当前 inode 下的 inode
>
> read_at 从当前的 inode 中读取
>
> write_at 在当前的 inode 中写
>
> clear 清空 inode

### 将 easy-fs 接入内核

内核需要实现一个 OSinode 并为其实现 File 特性

```
fn read(&self, buf: UserBuffer) -> usize;
fn write(&self, buf: UserBuffer) -> usize;
```

> 块设备驱动层，外设的寄存及通过映射到特定的物理地址来进行访问，在 boards 的qemu.rs 文件中定义了 MMIO 为 0x10001000 起始的 4KB，在创建内核的地址空间时，将这段内存添加进地址空间，直接使用 virtio-drivers 库

 virtio_blk.rs 

> VirtIO 设备需要占用部分内存作为一个公共区域从而更好的和 CPU 进行合作。这就像 MMU 需要在内存中保存多级页表才能和 CPU 共同实现分页机制一样。在 VirtIO 架构下，需要在公共区域中放置一种叫做 VirtQueue 的环形队列，CPU 可以向此环形队列中向 VirtIO 设备提交请求，也可以从队列中取得请求的结果

内核索引节点层

> OSinode 对 easy-fs 中的 inode 进行了封装，同时新增了 offset 以及读写标志，实现了 File 特性
>
> 同时声明了一个根节点 ROOT_INODE

文件描述符层

> 在进程的 TCB 中新增了文件描述符表 fd_table，是一个向量

文件系统初始化

> 1. 打开块设备 BLOCK_DEVICE 
> 2. 从块设备 BLOCK_DEVICE 上打开文件系统；
> 3. 从文件系统中获取根目录的 inode 

sys_open

> 先获取到进程的 token，然后得到内核地址空间下的 path，然后根据 path 和 flags 来找到或者创建一个 inode，之后在进程的 fd_table 中找到一个为空的项，把 inode 放进去，返回 inode 在 fd_table 中的下标

sys_close

> 将进程 fd_table 中的 fd 处清空

sys_exec

> 先 open_file 打开 ELF 文件，获取对应的 OSInode，然后再读取数据，构造进程